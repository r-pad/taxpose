

job_type: train_${task.name}_${task.phase.name}

# This has to happen before the benchmark stuff.
# This sets the data root for all data in the config.
# Something like autobot will override it.
data_root: /data


# TODO: Fix all the training scripts.

defaults:
  - _self_

  # Normal logging stuff.
  - _logging

  # Set up the model and checkpointing. Happens before dataset because order
  # matters for interpolation.
  - model: taxpose  # {taxpose, mlat_s256_vnn}

  # Task information for setting up the datamodule.
  - datamodule  # This datamodule will be used in evaluation, so factor it out.
  - benchmark: ndf  # {ndf, rlbench}
  - optional task: mug
  - optional phase: place
  - task/${benchmark}/${task}/task@task
  - task/${benchmark}/${task}/phase/${phase}@task.phase

training:
  batch_size: 8
  max_epochs: 500

  sigmoid_on: True

  # Optimizer Settings
  lr: 1e-4

  # Loss Settings
  flow_supervision: both
  displace_loss_weight: 1
  direct_correspondence_loss_weight: 1
  consistency_loss_weight: 0.1

  # Checkpoint Settings
  load_from_checkpoint: False
  checkpoint_file: null

  # Visualization Settings
  image_logging_period: 100
  log_every_n_steps: 100
  check_val_every_n_epoch: 5

seed: 0

resources:
  num_workers: 8

wandb:
  group: Null
